 1/1:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
 1/2:
def distance(x1, x2):
    #TODO
    d = np.sprt(((x1 - x2)**2).sum()) 
    return d
    
def knn(X_train, y_train, xt, k=7):
    #TODO
    pass
 1/3:
test_point = np.array([8, -4])

# Un-comment the line below and check if it comes out as 0.0  
# print(knn(data[:, :2], data[:, -1], test_point))
 1/4:
np.random.shuffle(data)
split = int(0.75 * data.shape[0])
# print split
train_data_X = data[:split, :2]
train_data_y = data[:split, -1]
test_data_X = data[split:, :2]
test_data_y = data[split:, -1]

print(train_data_X.shape, train_data_y.shape)
print(test_data_X.shape, test_data_y.shape)
 1/5:
# kx is the number of neighbours
def get_acc(kx):
    #TODO
    pass

#print(get_acc(7))
 1/6:
np.random.shuffle(data)
split = int(0.75 * data.shape[0])
# print split
train_data_X = data[:split, :2]
train_data_y = data[:split, -1]
test_data_X = data[split:, :2]
test_data_y = data[split:, -1]

print(train_data_X.shape, train_data_y.shape)
print(test_data_X.shape, test_data_y.shape)
 1/7:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

def run_d_n(dim,N_pts,L):
    pts=np.random.rand(N_pts,dim)-0.5 # simulate N_pts points on dim dimensions space
    ratio_list=[]
    for i in range(N_pts):
        # ignore the data point itself
        selected_pts=np.array([j for j in range(N_pts) if j!=i])
        # calculate the L2 or L1 distance with other points
        dist=np.linalg.norm(pts[selected_pts]-pts[i],L,axis=1)
        # calculate the ratio of the min. distance to the max. distance
        ratio=np.min(dist)/np.max(dist)
        ratio_list.append(ratio)
    # output the mean ratio
    return np.mean(ratio_list)

# Initialise the N_pts, the number of points we simulate
N_pts=200
# Setting l=2 to calculate the L2 distance
l=2
# Setting the number of dimensions we simulate
check_dim=range(1,550,50)
# Calculate the mean ratio on that dimension
ratio_list=[ run_d_n(dim,N_pts,l) for dim in check_dim]
# Plot the ratio with its corresponding dimension
plt.plot(check_dim,ratio_list)
plt.ylabel("Mean ratio of min/max pairwise distances")
plt.xlabel("Number of dimensions")
plt.title("Effect of increasing dimensionality on pairwise distances")
plt.xticks(np.arange(0, 600, step=100))
plt.show()
 1/8:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
 1/9:
mean_01 = np.array([1, 0.5])
cov_01 = np.array([[1, 0.1], [0.1, 1.2]])

mean_02 = np.array([4, 5])
cov_02 = np.array([[1, 0.1], [0.1, 1.2]])

dist_01 = np.random.multivariate_normal(mean_01, cov_01, 500)
dist_02 = np.random.multivariate_normal(mean_02, cov_02, 500)
print(dist_01.shape, dist_02.shape)
1/10:
plt.figure(0)
plt.xlim(-5, 10)
plt.ylim(-5, 10)

plt.scatter(dist_01[:, 0], dist_01[:, 1])
plt.scatter(dist_02[:, 0], dist_02[:, 1])#, color='red')
plt.show()
1/11:
r = dist_01.shape[0] + dist_02.shape[0]
c = dist_01.shape[1] + 1
data = np.zeros((r, c))
print(data.shape)

data[:dist_01.shape[0], :2] = dist_01
data[dist_01.shape[0]:, :2] = dist_02
data[dist_01.shape[0]:, -1] = 1.0

print(data.mean(axis=0))
1/12:
np.random.shuffle(data)
print(data[:10])
 2/1:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

def run_d_n(dim,N_pts,L):
    pts=np.random.rand(N_pts,dim)-0.5 # simulate N_pts points on dim dimensions space
    ratio_list=[]
    for i in range(N_pts):
        # ignore the data point itself
        selected_pts=np.array([j for j in range(N_pts) if j!=i])
        # calculate the L2 or L1 distance with other points
        dist=np.linalg.norm(pts[selected_pts]-pts[i],L,axis=1)
        # calculate the ratio of the min. distance to the max. distance
        ratio=np.min(dist)/np.max(dist)
        ratio_list.append(ratio)
    # output the mean ratio
    return np.mean(ratio_list)

# Initialise the N_pts, the number of points we simulate
N_pts=200
# Setting l=2 to calculate the L2 distance
l=2
# Setting the number of dimensions we simulate
check_dim=range(1,550,50)
# Calculate the mean ratio on that dimension
ratio_list=[ run_d_n(dim,N_pts,l) for dim in check_dim]
# Plot the ratio with its corresponding dimension
plt.plot(check_dim,ratio_list)
plt.ylabel("Mean ratio of min/max pairwise distances")
plt.xlabel("Number of dimensions")
plt.title("Effect of increasing dimensionality on pairwise distances")
plt.xticks(np.arange(0, 600, step=100))
plt.show()
 2/2:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
 2/3:
mean_01 = np.array([1, 0.5])
cov_01 = np.array([[1, 0.1], [0.1, 1.2]])

mean_02 = np.array([4, 5])
cov_02 = np.array([[1, 0.1], [0.1, 1.2]])

dist_01 = np.random.multivariate_normal(mean_01, cov_01, 500)
dist_02 = np.random.multivariate_normal(mean_02, cov_02, 500)
print(dist_01.shape, dist_02.shape)
 2/4:
plt.figure(0)
plt.xlim(-5, 10)
plt.ylim(-5, 10)

plt.scatter(dist_01[:, 0], dist_01[:, 1])
plt.scatter(dist_02[:, 0], dist_02[:, 1])#, color='red')
plt.show()
 2/5:
r = dist_01.shape[0] + dist_02.shape[0]
c = dist_01.shape[1] + 1
data = np.zeros((r, c))
print(data.shape)

data[:dist_01.shape[0], :2] = dist_01
data[dist_01.shape[0]:, :2] = dist_02
data[dist_01.shape[0]:, -1] = 1.0

print(data.mean(axis=0))
 2/6:
np.random.shuffle(data)
print(data[:10])
 2/7:
def distance(x1, x2):
    #TODO
    d = np.sprt(((x1 - x2)**2).sum()) 
    return d
    
def knn(X_train, y_train, xt, k=7):
    #TODO
    pass
 3/1:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
 3/2:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    xvals = # your code here 
    yvals = # your code here
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
 3/3:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlist
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,0.1*0.1)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
 3/4:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlist
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,0.1*0.1)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
 3/5:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
 3/6:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlist
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
 3/7:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
 3/8:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlist
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
 3/9:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlist
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
3/10:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/11:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
3/12:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/13:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
3/14:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    
    return D
3/15:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/16:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    print(D)
    return D
3/17:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
3/18:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 
    print(D)
    return D
3/19:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/20:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        f(xvals[i]) + np.random.normal(0,sigma*sigma)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
3/21:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/22:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
3/23:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/24:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
3/25:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
plt.tight_layout()
plt.show()
 5/1:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
 5/2:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
 5/3:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
 5/4:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(99):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
 5/5:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
 5/6:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        n = np.random.randint(0,10)
        randomlistx.append(n)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
 5/7:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
 5/8:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
 5/9:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
5/10:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
plt.tight_layout()
plt.show()
5/11:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 2))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0]*x
    # additonal dtype argument here avoids a depracation warning
    mods[i] = np.array([mod.intercept_[0], mod.coef_[0]], dtype='object')
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    if i==1: ax.legend()
    
fig.suptitle("$features: x$", fontsize=16)
plt.show()
5/12:
x0 = 5
x0_preds = np.zeros(9)
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    
    x0_preds[i] = lr(x0)
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
5/13:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = # your code here
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    mod = # fit model on dataset from previous line
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/14:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)


mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    #fsamples = f
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    mod = # fit model on dataset from previous line
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/15:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)
'''
# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
'''
mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    #fsamples = f_smapler2()
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X*X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/16:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)
'''
# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
'''
mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    #fsamples = f_smapler2()
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X, X*X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/17:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)
'''
# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
'''
mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    #fsamples = f_smapler2()
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X*X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/18:
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x+mods[i,2]*x**2
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    x0_preds[i] = lr(x0)
    
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
5/19:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_smapler2()
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X*X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/20:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler2()
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X*X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/21:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler2(f2, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X*X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/22:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler2(f2, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_2 = np.hstack((X, X**2))
    mod = LinearRegression().fit(X_2, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/23:
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x+mods[i,2]*x**2
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    x0_preds[i] = lr(x0)
    
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
5/24:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    mod = # your code here
    # your code here
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
5/25:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    '''
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    '''
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f2, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_2 = np.hstack((X, X**2))
    mod = LinearRegression().fit(X_2, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/26:
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x+mods[i,2]*x**2
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    x0_preds[i] = lr(x0)
    
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
5/27:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    mod = # your code here
    # your code here
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
5/28:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    #mod = # your code here
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_7 = np.hstack((X, X**2, X**3, X**4, X**5, X**6))
    mod = LinearRegression().fit(X_7, y)
    # your code here
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
5/29:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    #mod = # your code here
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    #X_7 = np.hstack((X, X**2, X**3, X**4, X**5, X**6))
    X_2 = np.hstack((X, X**2))
    X_3 = np.hstack((X_2, X**3))
    mod = LinearRegression().fit(X_3, y)
    # your code here
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
5/30:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    #mod = # your code here
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_7 = np.hstack((X, X**2, X**3, X**4, X**5, X**6))
    #X_2 = np.hstack((X, X**2))
    #X_3 = np.hstack((X_2, X**3))
    mod = LinearRegression().fit(X_7, y)
    # your code here
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
5/31:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    #mod = # your code here
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_7 = np.hstack((X, X**2, X**3, X**4, X**5, X**6))
    #X_2 = np.hstack((X, X**2))
    #X_3 = np.hstack((X_2, X**3))
    mod = LinearRegression().fit(X_7, y)
    # your code here
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2 + mod.coef_[0][2]*x**3 + mod.coef_[0][3]*x**4 + mod.coef_[0][4]*x**5 + mod.coef_[0][5]*x**6 
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
5/32:
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0] + mods[i,1]*x + mods[i,2]*x**2 +\
    mods[i,3]*x**3 + mods[i,4]*x**4 + mods[i,5]*x**5 + + mods[i,6]*x**6
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    x0_preds[i] = lr(x0)
    
plt.axvline(x0, color="red")
plt.ylim(-2,2)
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, -0.25), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.55), size=12, color='black')
plt.show()
5/33:
np.random.seed(10)

# true function
f = lambda x: 0.00002 * x**3 + 0.2 * np.cos(x**1.2)

fsamples = f_sampler(f, 100, sigma=0.1)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
5/34:
nmb_datasets = 80
xx_len = 1000
np.random.seed(10)

# generate datasets
datasets = np.array([f_sampler(f, 25, sigma=0.2) for _ in range(nmb_datasets)])
xx = np.linspace(1,10, xx_len)

preds = np.zeros(shape=(9, nmb_datasets, xx_len)) # store all predictions for all models

for deg in range(1, 10):
    Xs = np.array([D[:,0].reshape(-1,1) for D in datasets])
    ys = np.array([D[:,1].reshape(-1,1) for D in datasets])
    poly = PolynomialFeatures(degree=deg, include_bias=False)
    Xs = np.array([poly.fit_transform(X) for X in Xs])        # get polynomial feature matrix
    
    # fit models on all datasets
    lrs = np.array([LinearRegression().fit(X, y) for (X,y) in zip(Xs, ys)])
    
    # predictions on range xx 
    xx_poly = poly.fit_transform(xx.reshape(-1,1))
    preds[deg-1,:,:] = np.array([mod.predict(xx_poly) for mod in lrs]).reshape(nmb_datasets, xx_len)
5/35:
fig, ax = plt.subplots(3,3, figsize=(14,14))
truth = f(xx)
for j, ax in enumerate(ax.flat):
    avg_pred = np.mean(preds[j], axis=0)
    ax.plot(xx, truth, color="red", linewidth=4, zorder=20, label="truth")
    ax.plot(xx, avg_pred, color="orange", linewidth=4, zorder=20, label="$E(\\hat{f})$")    
    for i in range(nmb_datasets):
        ax.plot(xx, preds[j, i], alpha=0.3, color="blue", zorder=1)
        ax.set_title(f"polynomial degree: {j+1}")
        ax.set_ylim(-0.7,0.7)
    if j==1: ax.legend(loc="upper center")
plt.tight_layout()
plt.show()
5/36:
variances = np.zeros(shape=(9, xx_len))
bias_sq = np.zeros(shape=(9, xx_len))
for i in range(9):
    variances[i,:] = np.var(preds[i], axis=0)
    bias_sq[i,:] = (np.mean(preds[i], axis=0) - f(xx))**2
5/37:
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))
for i in range(9):
    ax1.plot(xx, bias_sq[i], label=f"degree {i+1}")
    ax2.plot(xx, variances[i])
    ax1.set_ylim(0,0.1)
    ax2.set_ylim(0,0.1)
    ax1.set_xlabel("x"); ax2.set_xlabel("x")
    ax1.set_title("Squared Bias")
    ax2.set_title("Variance")
ax1.legend(loc="upper center", ncol=3)
plt.tight_layout()
5/38:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f2, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_2 = np.hstack((X, X**2))
    mod = LinearRegression().fit(X_2, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
5/39:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    '''
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    '''
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f2, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_2 = np.hstack((X, X**2))
    mod = LinearRegression().fit(X_2, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
 6/1:
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x+mods[i,2]*x**2
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    x0_preds[i] = lr(x0)
    
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
 6/2:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
 6/3:
# true function
f = lambda x: 0.001 * x**3

def f_sampler(f, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    
    # build dataset D
    D = np.zeros(shape=(n, 2))
    D[:,0] = xvals; D[:,1] = yvals; 

    return D
 6/4:
fsamples = f_sampler(f, 100, sigma=0.2)

xx = np.linspace(1,10,1000)
plt.plot(xx, f(xx), label="truth")
plt.scatter(*fsamples.T, color="red", label="samples")
plt.legend()
plt.show()
 6/5:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
plt.tight_layout()
plt.show()
 6/6:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 2))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    mod = LinearRegression().fit(X, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0]*x
    # additonal dtype argument here avoids a depracation warning
    mods[i] = np.array([mod.intercept_[0], mod.coef_[0]], dtype='object')
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    if i==1: ax.legend()
    
fig.suptitle("$features: x$", fontsize=16)
plt.show()
 6/7:
x0 = 5
x0_preds = np.zeros(9)
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    
    x0_preds[i] = lr(x0)
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
 6/8:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3

def f_sampler2(f2, n=100, sigma=0.1):    
    # sample points from function f with Gaussian noise (0,sigma**2)
    randomlistx = []
    for i in range(n):
        d = np.random.randint(0,10)
        randomlistx.append(d)
    xvals = randomlistx
    y = []
    for i in range(n):
        value = f2(xvals[i]) + np.random.normal(0,sigma*sigma)
        y.append(value)
    yvals = y
    '''
    x_2 = []
    for i in range(n):
        x_2.append(xvals[i]**2)
    '''
    # build dataset D
    D = np.zeros(shape=(n, 3))
    D[:,0] = xvals; D[:,1] = yvals; D[:,2] = x_2

    return D

mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f2, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_2 = np.hstack((X, X**2))
    mod = LinearRegression().fit(X_2, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
 6/9:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

# true function
f2 = lambda x: 0.001 * x**3


mods = np.zeros((9, 3))                                  # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here, Create dataset with extra features
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_2 = np.hstack((X, X**2))
    mod = LinearRegression().fit(X_2, y)
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
fig.suptitle("$features: x, x^2$", fontsize=16)
plt.show()
6/10:
plt.plot(xx, f(xx))
for i in range(9):
    lr = lambda x: mods[i,0]+mods[i,1]*x+mods[i,2]*x**2
    plt.plot(xx, lr(xx), color="green", alpha=0.4)
    x0_preds[i] = lr(x0)
    
plt.axvline(x0, color="red")
plt.annotate(f'Bias Squared: {round((np.mean(x0_preds)-f(x0))**2,4)}', (6, 0.05), size=12, color='black')
plt.annotate(f'Variance: {round(np.var(x0_preds),4)}', (6, -0.05), size=12, color='black')
plt.show()
6/11:
fig, ax = plt.subplots(3,3, figsize=(10,10))
np.random.seed(10)

mods = np.zeros((9, 7))      # store models
for i, ax in enumerate(ax.flat):
    ax.plot(xx, f(xx), label="truth")
    fsamples = f_sampler(f, 25, sigma=0.2)
    ax.scatter(*fsamples.T, color="red", label="samples")
    ax.set_title(f"$D_{i+1}$")
    
    # build model
    # your code here
    #mod = # your code here
    X = fsamples[:,0].reshape(-1,1)
    y = fsamples[:,1].reshape(-1,1)
    X_7 = np.hstack((X, X**2, X**3, X**4, X**5, X**6))
    #X_2 = np.hstack((X, X**2))
    #X_3 = np.hstack((X_2, X**3))
    mod = LinearRegression().fit(X_7, y)
    # your code here
    lr = lambda x: mod.intercept_[0] + mod.coef_[0][0]*x + mod.coef_[0][1]*x**2 + mod.coef_[0][2]*x**3 + mod.coef_[0][3]*x**4 + mod.coef_[0][4]*x**5 + mod.coef_[0][5]*x**6 
    mods[i] = np.array([mod.intercept_[0], *mod.coef_[0]])
    ax.plot(xx, lr(xx), color="green", label="$\\hat{f}$")
    
fig.suptitle("$features: x, x^2, x^3, x^4, x^5, x^6$", fontsize=16)
plt.show()
 7/1:
import numpy as np
import matplotlib.pyplot as plt

def generate_data(n=20, means=[[3,3],[-1,1]], seed=1):
    '''
    generate data from two gaussians
    '''
    np.random.seed(seed)
    m1 = np.array(means[0])
    m2 = np.array(means[1])
    S1 = np.random.rand(2,2)
    S2 = np.random.rand(2,2)
    dist_01 = np.random.multivariate_normal(m1, S1.T @ S1, n)
    dist_02 = np.random.multivariate_normal(m2, S2.T @ S2, n)
    X = np.concatenate((np.ones(2*n).reshape(-1,1), 
                        np.concatenate((dist_01, dist_02))), axis=1)
    y = np.concatenate((np.ones(n), np.zeros(n))).reshape(-1,1)
    shuffle_idx = np.random.choice(2*n, size=2*n, replace=False)
    X = X[shuffle_idx]
    y = y[shuffle_idx]
    return X, y

def plot_perceptron(ax, X, y, w):
    pos_points = X[np.where(y==1)[0]]
    neg_points = X[np.where(y==0)[0]]
    ax.scatter(pos_points[:, 1], pos_points[:, 2], color='blue')
    ax.scatter(neg_points[:, 1], neg_points[:, 2], color='red')
    xx = np.linspace(-6,6)
    yy = -w[0]/w[2] - w[1]/w[2] * xx
    ax.plot(xx, yy, color='orange')
    
    ratio = (w[2]/w[1] + w[1]/w[2])
    xpt = (-1*w[0] / w[2]) * 1/ratio
    ypt = (-1*w[0] / w[1]) * 1/ratio
    
    ax.arrow(xpt, ypt, w[1], w[2], head_width=0.2, color='orange')
    ax.axis('equal')
 7/2:
def sigmoid(x):
    return 1/(1+np.exp(-1*x))

def grad_sigmoid(x):
    return sigmoid(x) * (1-sigmoid(x))
 7/3:
# plot sigmoid functions 
xx = np.linspace(-6, 6, 1000)
plt.plot(xx, sigmoid(xx), label='$\sigma(x)$')
plt.plot(xx, grad_sigmoid(xx), label='$\sigma\'(x)$')
plt.legend()
plt.show()
 7/4:
def loss_i(w, x_i, y_i):
    '''cross entropy loss for i-th data point'''
    # your code here
    return # your code here
    
def grad_loss_i(w, x_i, y_i):
    '''grad loss for i-th data point'''
    # your code here
    return # your code here

def gradient_descent(X, y, eta, T):
    N = X.shape[0]
    w = np.array([0,0,0]) # init w
    for t in range(T):
        loss = 0
        grad_loss = 0
        for i in range(N):
            loss += loss_i(w, X[i], y[i])
            grad_loss += grad_loss_i(w, X[i], y[i])
        print(f"loss = {loss}")
        w = w - eta * grad_loss
        fig, ax = plt.subplots()
        plot_perceptron(ax, X, y, w)
        plt.show()
 7/5:
np.random.seed(12)
X, y = generate_data(n=40, means=[[2,1],[1,0]])
gradient_descent(X, y, 0.1, 20)
 8/1:
import numpy as np
import matplotlib.pyplot as plt

def generate_data(n=20, means=[[3,3],[-1,1]], seed=1):
    '''
    generate data from two gaussians
    '''
    np.random.seed(seed)
    m1 = np.array(means[0])
    m2 = np.array(means[1])
    S1 = np.random.rand(2,2)
    S2 = np.random.rand(2,2)
    dist_01 = np.random.multivariate_normal(m1, S1.T @ S1, n)
    dist_02 = np.random.multivariate_normal(m2, S2.T @ S2, n)
    X = np.concatenate((np.ones(2*n).reshape(-1,1), 
                        np.concatenate((dist_01, dist_02))), axis=1)
    y = np.concatenate((np.ones(n), np.zeros(n))).reshape(-1,1)
    shuffle_idx = np.random.choice(2*n, size=2*n, replace=False)
    X = X[shuffle_idx]
    y = y[shuffle_idx]
    return X, y

def plot_perceptron(ax, X, y, w):
    pos_points = X[np.where(y==1)[0]]
    neg_points = X[np.where(y==0)[0]]
    ax.scatter(pos_points[:, 1], pos_points[:, 2], color='blue')
    ax.scatter(neg_points[:, 1], neg_points[:, 2], color='red')
    xx = np.linspace(-6,6)
    yy = -w[0]/w[2] - w[1]/w[2] * xx
    ax.plot(xx, yy, color='orange')
    
    ratio = (w[2]/w[1] + w[1]/w[2])
    xpt = (-1*w[0] / w[2]) * 1/ratio
    ypt = (-1*w[0] / w[1]) * 1/ratio
    
    ax.arrow(xpt, ypt, w[1], w[2], head_width=0.2, color='orange')
    ax.axis('equal')
 8/2:
def sigmoid(x):
    return 1/(1+np.exp(-1*x))

def grad_sigmoid(x):
    return sigmoid(x) * (1-sigmoid(x))
 8/3:
# plot sigmoid functions 
xx = np.linspace(-6, 6, 1000)
plt.plot(xx, sigmoid(xx), label='$\sigma(x)$')
plt.plot(xx, grad_sigmoid(xx), label='$\sigma\'(x)$')
plt.legend()
plt.show()
 9/1:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

import pandas as pd
# pandas.__version__
10/1:
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

import pandas as pd
# pandas.__version__
10/2:
rng = np.random.RandomState(1)
A=rng.rand(2, 2)
B=rng.randn(2, 200)
X = np.dot(A,B).T
plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal')
10/3:
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(X)
10/4: print(pca.components_)
10/5: print(pca.explained_variance_)
10/6:
def draw_vector(v0, v1, ax=None):
    ax = ax or plt.gca()
    arrowprops=dict(arrowstyle='->',
                    linewidth=2,
                    shrinkA=0, shrinkB=0)
    ax.annotate('', v1, v0, arrowprops=arrowprops)

# plot data
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
for length, vector in zip(pca.explained_variance_, pca.components_):
    print(length,vector)
    v = vector * 3 * np.sqrt(length)
    draw_vector(pca.mean_, pca.mean_ - v)
plt.axis('equal')
10/7:
pca = PCA(n_components=1)
pca.fit(X)
X_pca = pca.transform(X)
print("original shape:   ", X.shape)
print("transformed shape:", X_pca.shape)
10/8:
X_new = pca.inverse_transform(X_pca)
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)
plt.axis('equal');
10/9:
from sklearn.datasets import load_digits
digits = load_digits()
digits.data.shape
10/10:
pca = PCA(n_components=2)  # project from 64 to 2 dimensions
projected = pca.fit_transform(digits.data)
print(digits.data.shape)
print(projected.shape)
10/11:
import matplotlib.cm

plt.scatter(projected[:, 0], projected[:, 1],
            c=digits.target, edgecolor='none', alpha=0.4,
            # cmap=plt.cm.get_cmap('spectral', 10))
            cmap=plt.cm.get_cmap('Spectral', 10))
plt.xlabel('component 1')
plt.ylabel('component 2')
plt.colorbar();
11/1:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
12/1:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
13/1:
# FIRST STEP: Knowing what each action does (Observing)

observation = env.reset()                     # Game begins
obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs 
state = np.stack((obs, obs), axis=1)
done = False
for t in range(observetime):
    if np.random.rand() <= epsilon:
        action = np.random.randint(0, env.action_space.n, size=1)[0]
    else:
        Q = model.predict(state)          # Q-values predictions
        action = np.argmax(Q)             # Move with highest Q-value is the chosen one
    observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action
    obs_new = np.expand_dims(observation_new, axis=0)          # (Formatting issues)
    state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     # Update the input with the new state of the game
    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence
    state = state_new         # Update state
    if done:
        env.reset()           # Restart game if it's finished
        obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs 
        state = np.stack((obs, obs), axis=1)
print('Observing Finished')
13/2:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
13/3:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
15/1:
# FIRST STEP: Knowing what each action does (Observing)

observation = env.reset()                     # Game begins
obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs 
state = np.stack((obs, obs), axis=1)
done = False
for t in range(observetime):
    if np.random.rand() <= epsilon:
        action = np.random.randint(0, env.action_space.n, size=1)[0]
    else:
        Q = model.predict(state)          # Q-values predictions
        action = np.argmax(Q)             # Move with highest Q-value is the chosen one
    observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action
    obs_new = np.expand_dims(observation_new, axis=0)          # (Formatting issues)
    state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     # Update the input with the new state of the game
    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence
    state = state_new         # Update state
    if done:
        env.reset()           # Restart game if it's finished
        obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs 
        state = np.stack((obs, obs), axis=1)
print('Observing Finished')
15/2:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
17/1:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
19/1:
# FIRST STEP: Knowing what each action does (Observing)

observation = env.reset()                     # Game begins
obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs 
state = np.stack((obs, obs), axis=1)
done = False
for t in range(observetime):
    if np.random.rand() <= epsilon:
        action = np.random.randint(0, env.action_space.n, size=1)[0]
    else:
        Q = model.predict(state)          # Q-values predictions
        action = np.argmax(Q)             # Move with highest Q-value is the chosen one
    observation_new, reward, done, info = env.step(action)     # See state of the game, reward... after performing the action
    obs_new = np.expand_dims(observation_new, axis=0)          # (Formatting issues)
    state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     # Update the input with the new state of the game
    D.append((state, action, reward, state_new, done))         # 'Remember' action and consequence
    state = state_new         # Update state
    if done:
        env.reset()           # Restart game if it's finished
        obs = np.expand_dims(observation, axis=0)     # (Formatting issues) Making the observation the first element of a batch of inputs 
        state = np.stack((obs, obs), axis=1)
print('Observing Finished')
20/1:
# INITIALIZATION: libraries, parameters, network...

from keras.models import Sequential      # One layer after the other
from keras.layers import Dense, Flatten  # Dense layers are fully connected layers, Flatten layers flatten out multidimensional inputs
from collections import deque            # For storing moves 

import numpy as np
import gym                                # To train our network
env = gym.make('ppaquette/SuperMarioBros-1-1-v0')          # Choose game (any in the gym should work)

import random     # For sampling batches from the observations


# Create network. Input is two consecutive game states, output is Q-values of the possible moves.
model = Sequential()
model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))
model.add(Flatten())       # Flatten input so as to have no problems with processing
model.add(Dense(18, init='uniform', activation='relu'))
model.add(Dense(10, init='uniform', activation='relu'))
model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    # Same number of outputs as possible actions

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Parameters
D = deque()                                # Register where the actions will be stored

observetime = 500                          # Number of timesteps we will be acting on the game and observing results
epsilon = 0.7                              # Probability of doing a random move
gamma = 0.9                                # Discounted future reward. How much we care about steps further in time
mb_size = 50                               # Learning minibatch size
22/1:
# load data
with open("data/X.npy", "rb") as f:
    X = np.load(f, allow_pickle=True)
with open("data/y.npy", "rb") as f:
    y = np.load(f, allow_pickle=True)
23/1:
import os
import random
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pickle
23/2:
# load data
with open("data/X.npy", "rb") as f:
    X = np.load(f, allow_pickle=True)
with open("data/y.npy", "rb") as f:
    y = np.load(f, allow_pickle=True)
23/3:
# load labels
with open("data/labels.pkl", "rb") as f:
    labels = pickle.load(f)
with open("data/inv_labels.pkl", "rb") as f:
    inv_labels = pickle.load(f)
23/4:
print(X.shape)
print(y.shape)
23/5:
print(len(labels))
print(len(inv_labels))
23/6:
for i in range(13225, 13233):
    print(X[i][56][47])
23/7:
# use to check if image matches
def show_img_person(name):
    name = name.replace(' ', '_')
    img_dir = os.path.join("lfw_funneled", name)
    img_path = os.path.join("lfw_funneled", name, os.listdir(img_dir)[0])
    img = mpimg.imread(img_path)
    imgplot = plt.imshow(img)
    plt.show()
23/8: # run below 2 cells
23/9:
k = random.randint(0, 13232)
plt.imshow(X[k]), labels[y[k]]
23/10: show_img_person(labels[y[k]])
23/11:
k = random.randint(0, 13232)
plt.imshow(X[k])#, labels[y[k]]
23/12:
k = random.randint(0, 13232)
plt.imshow(X[k])#, labels[y[k]]
23/13:
k = random.randint(0, 13232)
plt.imshow(X[k]), labels[y[k]]
23/14: show_img_person(labels[y[k]])
23/15:
k = random.randint(0, 13232)
plt.imshow(X[k])# labels[y[k]]
23/16: show_img_person(labels[y[k]])
23/17: print(labels.shape())
23/18: print(labels[:3])
23/19:
i = 0
for key in labels:
    print(key)
    i = i + 1
    if i == 10:
        break
23/20:
i = 0
for key in labels:
    print(labels[key])
    i = i + 1
    if i == 10:
        break
23/21:
i = 0
for key in labels:
    print(labels)
    i = i + 1
    if i == 10:
        break
23/22: print(labels)
23/23: print(labels)
23/24: print(inv_labels)
23/25: print(labels)
23/26: print(labels)
23/27: print(inv_labels)
23/28: print(X[1])
25/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class VGG_Face(nn.Module):
    """
    Main Class
    """

    def __init__(self, useEmbedding=False):
        """
        Constructor
        """
        super().__init__()
        self.useEmbedding = useEmbedding
        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)
        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)
        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)
        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.fc6 = nn.Linear(512 * 7 * 7, 4096)
        self.fc7 = nn.Linear(4096, 4096)
        # Softmax
        self.fc8_1 = nn.Linear(4096, 5749)
        # Feature embedding
        self.fc8_2 = nn.Linear(4096, 1024)

    def forward(self, x):
            """ Pytorch forward
            Args:
                x: input image (224x224)
            Returns: class logits
            """
            x = F.relu(self.conv_1_1(x))
            x = F.relu(self.conv_1_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_2_1(x))
            x = F.relu(self.conv_2_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_3_1(x))
            x = F.relu(self.conv_3_2(x))
            x = F.relu(self.conv_3_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_4_1(x))
            x = F.relu(self.conv_4_2(x))
            x = F.relu(self.conv_4_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_5_1(x))
            x = F.relu(self.conv_5_2(x))
            x = F.relu(self.conv_5_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc6(x))
            x = F.dropout(x, 0.5, self.training)
            x = F.relu(self.fc7(x))
            x = F.dropout(x, 0.5, self.training)
            if self.useEmbedding:
                x = self.fc8_2(x)
                x = F.normalize(x, p=2, dim=1)
            else:
                x = self.fc8_1(x)
                x = F.softmax(x, dim=0)
            return x
25/2:
X = torch.from_numpy(X_np).permute(0, 3, 1, 2).to(device)
y = torch.from_numpy(y_np).to(device)
25/3:
with open("data/lfw/X.npy", "rb") as f:
    X_np = np.load(f, allow_pickle=True)
with open("data/lfw/y.npy", "rb") as f:
    y_np = np.load(f, allow_pickle=True)
25/4:
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class VGG_Face(nn.Module):
    """
    Main Class
    """

    def __init__(self, useEmbedding=False):
        """
        Constructor
        """
        super().__init__()
        self.useEmbedding = useEmbedding
        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)
        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)
        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)
        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.fc6 = nn.Linear(512 * 7 * 7, 4096)
        self.fc7 = nn.Linear(4096, 4096)
        # Softmax
        self.fc8_1 = nn.Linear(4096, 5749)
        # Feature embedding
        self.fc8_2 = nn.Linear(4096, 1024)

    def forward(self, x):
            """ Pytorch forward
            Args:
                x: input image (224x224)
            Returns: class logits
            """
            x = F.relu(self.conv_1_1(x))
            x = F.relu(self.conv_1_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_2_1(x))
            x = F.relu(self.conv_2_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_3_1(x))
            x = F.relu(self.conv_3_2(x))
            x = F.relu(self.conv_3_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_4_1(x))
            x = F.relu(self.conv_4_2(x))
            x = F.relu(self.conv_4_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_5_1(x))
            x = F.relu(self.conv_5_2(x))
            x = F.relu(self.conv_5_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc6(x))
            x = F.dropout(x, 0.5, self.training)
            x = F.relu(self.fc7(x))
            x = F.dropout(x, 0.5, self.training)
            if self.useEmbedding:
                x = self.fc8_2(x)
                x = F.normalize(x, p=2, dim=1)
            else:
                x = self.fc8_1(x)
                x = F.softmax(x, dim=0)
            return x
25/5: IMG_SIZE = 224
25/6:
class LinearClassifierTrainer(object):
    def __init__(self, model, criterion, optimizer, device):
        self.criterion=criterion
        self.optimizer=optimizer
        self.device=device
        self.model = model

    def train(self,dataloader,epoch):
        accuracy = 0
        
        for i in tqdm(range(epoch)):
            total=0
            correct=0
            for batch_id, (data,target) in enumerate(dataloader):
                #if self.cuda:
                 #   data, target = data.cuda(), target.cuda()

                data = data.float().to(device)
                target = target.type(torch.LongTensor).to(device)

                output = self.model(data)  

                loss = self.criterion(output, target)
                loss.backward()             # compute gradients
                self.optimizer.step()         # update weights
                pred = torch.argmax(output, dim=1).float()
                correct += (pred == target).float().sum().item()
                total += target.size()[0]
            
            if i == epoch - 1:
                accuracy = correct/total
            
        return accuracy
25/7:
with open("data/lfw/X.npy", "rb") as f:
    X_np = np.load(f, allow_pickle=True)
with open("data/lfw/y.npy", "rb") as f:
    y_np = np.load(f, allow_pickle=True)
26/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class VGG_Face(nn.Module):
    """
    Main Class
    """

    def __init__(self, useEmbedding=False):
        """
        Constructor
        """
        super().__init__()
        self.useEmbedding = useEmbedding
        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)
        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)
        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)
        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.fc6 = nn.Linear(512 * 7 * 7, 4096)
        self.fc7 = nn.Linear(4096, 4096)
        # Softmax
        self.fc8_1 = nn.Linear(4096, 5749)
        # Feature embedding
        self.fc8_2 = nn.Linear(4096, 1024)

    def forward(self, x):
            """ Pytorch forward
            Args:
                x: input image (224x224)
            Returns: class logits
            """
            x = F.relu(self.conv_1_1(x))
            x = F.relu(self.conv_1_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_2_1(x))
            x = F.relu(self.conv_2_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_3_1(x))
            x = F.relu(self.conv_3_2(x))
            x = F.relu(self.conv_3_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_4_1(x))
            x = F.relu(self.conv_4_2(x))
            x = F.relu(self.conv_4_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_5_1(x))
            x = F.relu(self.conv_5_2(x))
            x = F.relu(self.conv_5_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc6(x))
            x = F.dropout(x, 0.5, self.training)
            x = F.relu(self.fc7(x))
            x = F.dropout(x, 0.5, self.training)
            if self.useEmbedding:
                x = self.fc8_2(x)
                x = F.normalize(x, p=2, dim=1)
            else:
                x = self.fc8_1(x)
                x = F.softmax(x, dim=0)
            return x
26/2: IMG_SIZE = 224
26/3:
class LinearClassifierTrainer(object):
    def __init__(self, model, criterion, optimizer, device):
        self.criterion=criterion
        self.optimizer=optimizer
        self.device=device
        self.model = model

    def train(self,dataloader,epoch):
        accuracy = 0
        
        for i in tqdm(range(epoch)):
            total=0
            correct=0
            for batch_id, (data,target) in enumerate(dataloader):
                #if self.cuda:
                 #   data, target = data.cuda(), target.cuda()

                data = data.float().to(device)
                target = target.type(torch.LongTensor).to(device)

                output = self.model(data)  

                loss = self.criterion(output, target)
                loss.backward()             # compute gradients
                self.optimizer.step()         # update weights
                pred = torch.argmax(output, dim=1).float()
                correct += (pred == target).float().sum().item()
                total += target.size()[0]
            
            if i == epoch - 1:
                accuracy = correct/total
            
        return accuracy
26/4:
with open("data/lfw/X.npy", "rb") as f:
    X_np = np.load(f, allow_pickle=True)
with open("data/lfw/y.npy", "rb") as f:
    y_np = np.load(f, allow_pickle=True)
26/5:
with open("data/X.npy", "rb") as f:
    X_np = np.load(f, allow_pickle=True)
with open("data/y.npy", "rb") as f:
    y_np = np.load(f, allow_pickle=True)
26/6:
X = torch.from_numpy(X_np).permute(0, 3, 1, 2).to(device)
y = torch.from_numpy(y_np).to(device)
26/7:
print(X.shape())
type(X)
26/8:
#print(X.shape())
type(X)
26/9:
print(X.size())
type(X)
26/10:
train_dataset = torch.utils.data.TensorDataset(X, y)
train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=1)
26/11:
# Train the model
net = VGG_Face().to(device)
optimizer = torch.optim.SGD(net.parameters(),lr=0.01, momentum=0.9,
                                 weight_decay=0.005)
loss=nn.CrossEntropyLoss()
trainer = LinearClassifierTrainer( net, loss, optimizer, device )
26/12:
num_epochs = 100
accuracy = trainer.train(train_loader, num_epochs)
28/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class VGG_Face(nn.Module):
    """
    Main Class
    """

    def __init__(self, useEmbedding=False):
        """
        Constructor
        """
        super().__init__()
        self.useEmbedding = useEmbedding
        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)
        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)
        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)
        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)
        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
        self.fc6 = nn.Linear(512 * 7 * 7, 4096)
        self.fc7 = nn.Linear(4096, 4096)
        # Softmax
        self.fc8_1 = nn.Linear(4096, 5749)
        # Feature embedding
        self.fc8_2 = nn.Linear(4096, 1024)

    def forward(self, x):
            """ Pytorch forward
            Args:
                x: input image (224x224)
            Returns: class logits
            """
            x = F.relu(self.conv_1_1(x))
            x = F.relu(self.conv_1_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_2_1(x))
            x = F.relu(self.conv_2_2(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_3_1(x))
            x = F.relu(self.conv_3_2(x))
            x = F.relu(self.conv_3_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_4_1(x))
            x = F.relu(self.conv_4_2(x))
            x = F.relu(self.conv_4_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = F.relu(self.conv_5_1(x))
            x = F.relu(self.conv_5_2(x))
            x = F.relu(self.conv_5_3(x))
            x = F.max_pool2d(x, 2, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc6(x))
            x = F.dropout(x, 0.5, self.training)
            x = F.relu(self.fc7(x))
            x = F.dropout(x, 0.5, self.training)
            if self.useEmbedding:
                x = self.fc8_2(x)
                x = F.normalize(x, p=2, dim=1)
            else:
                x = self.fc8_1(x)
                x = F.softmax(x, dim=0)
            return x
28/2: IMG_SIZE = 224
28/3:
class LinearClassifierTrainer(object):
    def __init__(self, model, criterion, optimizer, device):
        self.criterion=criterion
        self.optimizer=optimizer
        self.device=device
        self.model = model

    def train(self,dataloader,epoch):
        accuracy = 0
        
        for i in tqdm(range(epoch)):
            total=0
            correct=0
            for batch_id, (data,target) in enumerate(dataloader):
                #if self.cuda:
                 #   data, target = data.cuda(), target.cuda()

                data = data.float().to(device)
                target = target.type(torch.LongTensor).to(device)

                output = self.model(data)  

                loss = self.criterion(output, target)
                loss.backward()             # compute gradients
                self.optimizer.step()         # update weights
                pred = torch.argmax(output, dim=1).float()
                correct += (pred == target).float().sum().item()
                total += target.size()[0]
            
            if i == epoch - 1:
                accuracy = correct/total
            
        return accuracy
28/4:
with open("data/X.npy", "rb") as f:
    X_np = np.load(f, allow_pickle=True)
with open("data/y.npy", "rb") as f:
    y_np = np.load(f, allow_pickle=True)
28/5:
X = torch.from_numpy(X_np).permute(0, 3, 1, 2).to(device)
y = torch.from_numpy(y_np).to(device)
28/6:
print(X.size())
type(X)
28/7:
train_dataset = torch.utils.data.TensorDataset(X, y)
train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=1)
28/8:
# Train the model
net = VGG_Face().to(device)
optimizer = torch.optim.SGD(net.parameters(),lr=0.01, momentum=0.9,
                                 weight_decay=0.005)
loss=nn.CrossEntropyLoss()
trainer = LinearClassifierTrainer( net, loss, optimizer, device )
28/9:
num_epochs = 100
accuracy = trainer.train(train_loader, num_epochs)
33/1: !pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib
33/2: !pip install tensorflow==2.5.1 tensorflow-gpu==2.4.1 opencv-python matplotlib
33/3: !pip install tensorflow==2.5.1 tensorflow-gpu==2.4.1 opencv-python matplotlib
33/4: !pip install tensorflow==2.5.1 tensorflow-gpu==2.4.1 opencv-python matplotlib
33/5: !pip install tensorflow==2.5.1 opencv-python matplotlib #tensorflow-gpu==2.4.1
33/6:
# Import standard dependencies
import cv2
import os
import random
import numpy as np
from matplotlib import pyplot as plt
33/7: !pip install tensorflow==2.5.1 opencv-python matplotlib #tensorflow-gpu==2.4.1
33/8:
# Import standard dependencies
import cv2
import os
import random
import numpy as np
from matplotlib import pyplot as plt
33/9:
# Import tensorflow dependencies - Functional API
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten
import tensorflow as tf
34/1:
# Import tensorflow dependencies - Functional API
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten
import tensorflow as tf
35/1: !pip install tensorflow==2.5.1 opencv-python matplotlib #tensorflow-gpu==2.4.1
35/2:
# Import standard dependencies
import cv2
import os
import random
import numpy as np
from matplotlib import pyplot as plt
35/3:
# Import tensorflow dependencies - Functional API
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten
import tensorflow as tf
36/1:
# Import tensorflow dependencies - Functional API
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten
import tensorflow as tf
38/1:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        img0, img1, label = img0, img1, label #img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
38/2: torch.save(model.state_dict(), PATH)
38/3: torch.save(net.state_dict(), '/model')
38/4:
model_scripted = torch.jit.script(net) # Export to TorchScript
model_scripted.save('model_scripted.pt') # Save
38/5:
model = torch.jit.load('model_scripted.pt')
model.eval()
38/6: siamese_dataset.__getitem__()
38/7: siamese_dataset.__getitem__(2)
38/8:
out1, out2 = siamese_dataset.__getitem__(2)
print(out1)
print('------------------------')
print(out2)
38/9:
out1 = siamese_dataset.__getitem__(2)
print(out1)
print('------------------------')
print(out2)
38/10:
out1 = siamese_dataset.__getitem__(2)
print(type(out1))
print('------------------------')
38/11:
(out1, out2) = siamese_dataset.__getitem__(2)
print(type(out1))
print('------------------------')
38/12:
out1 = siamese_dataset.__getitem__(2)
print(out1.size())
print('------------------------')
38/13:
out1 = siamese_dataset.__getitem__(2)
print(out1.len())
print('------------------------')
38/14:
out1 = siamese_dataset.__getitem__(2)
print(len(out1)
print('------------------------')
38/15:
out1 = siamese_dataset.__getitem__(2)
print(len(out1))
print('------------------------')
38/16:
(img1, img2, three) = siamese_dataset.__getitem__(2)
print(type(three))
print(three)
print('------------------------')
38/17:

output1, output2 = model(img0, img1)
euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
print(euclidean)
38/18:
(img1, img2, three) = siamese_dataset.__getitem__(3)
print(type(three))
print(three)
print('------------------------')
38/19:
(img1, img2, three) = siamese_dataset.__getitem__(3)
print(type(three))
print(three)
print('------------------------')
38/20:

output1, output2 = model(img1, img2)
euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
print(euclidean)
38/21:

output1, output2 = net(img1, img2)
euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
print(euclidean)
38/22:
for i, (img0, img1, label) in enumerate(train_dataloader, 0)
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
    print(euclidean)
    break
38/23:
for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
    print(euclidean)
    break
38/24:
for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
    print(euclidean_distance)
    break
38/25:
for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
    loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
    print(loss_contrastive)
    break
38/26:
for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
    loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(2 - euclidean_distance, min=0.0), 2))
    print(loss_contrastive)
    break
38/27:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./data/faces/testing/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)

for i in range(5):
    # Iterate over 5 images and test them with the first image (x0)
    _, x1, label2 = next(dataiter)

    # Concatenate the two images together
    concatenated = torch.cat((x0, x1), 0)
    
    output1, output2 = net(x0.cuda(), x1.cuda())
    euclidean_distance = F.pairwise_distance(output1, output2)
    imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
38/28:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./data/faces/testing/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)

for i in range(5):
    # Iterate over 5 images and test them with the first image (x0)
    _, x1, label2 = next(dataiter)

    # Concatenate the two images together
    concatenated = torch.cat((x0, x1), 0)
    
    output1, output2 = net(x0, x1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
38/29:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./data/faces/testing/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)
print('1 -----')
for i in range(5):
    print('2 -----')
    # Iterate over 5 images and test them with the first image (x0)
    _, x1, label2 = next(dataiter)
    print('3 -----')
    # Concatenate the two images together
    concatenated = torch.cat((x0, x1), 0)
    
    output1, output2 = net(x0, x1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
38/30:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./data/faces/testing/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)
print('1 -----')
for i in range(5):
    print('2 -----')
    # Iterate over 5 images and test them with the first image (x0)
    x0, x1, label2 = next(dataiter)
    print('3 -----')
    # Concatenate the two images together
    concatenated = torch.cat((x0, x1), 0)
    
    output1, output2 = net(x0, x1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
38/31:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(28, 28)
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/32:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    happy_array = np.random.randn(3, 3)
    print(type(happy_array))
    print(happy_array)
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/33:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    print(img0)
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/34:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    print(type(img0)
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/35:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    print(type(img0))
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/36:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    print(type(img0))
    print(img0.size())
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/37:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    print(hundred_times_hundred)
    im = plt.imshow(img0, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/38:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    print(hundred_times_hundred)
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item)
    break
38/39:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item())
    break
38/40:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(euclidean_distance.item())
    break
38/41:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(type(euclidean_distance))
    print(euclidean_distance.item())
    break
38/42:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(type(euclidean_distance))
    print(euclidean_distance.item())
    break
38/43:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(type(euclidean_distance))
    print(euclidean_distance.size())
    print(euclidean_distance.item())
    break
38/44:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    print(type(euclidean_distance))
    print(euclidean_distance.size())
    print(output1.size())
    print(euclidean_distance.item())
    break
38/45:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    print(euclidean_distance[:4])
    break
38/46:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    print(euclidean_distance[:4])
    print(output1[:4])
    break
38/47:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    break
38/48:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/49:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/50:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/51:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
   
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/52:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/53:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/54:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[2][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/55:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    break
38/56:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = TheModelClass(*args, **kwargs)
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance))
    break
38/57:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance))
    break
38/58:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance)[0])
    break
38/59:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance))
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/60:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/61:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0[0][0], img1[0][0])
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/62:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0[0], img1[0])
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/63:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/64:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/65:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/66:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/67:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/68:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model([[img0[0][0]]], [[img1[0][0]]])
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(torch.sum(euclidean_distance).item())
    break
38/69:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output1[1][0], output1[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(np.sqrt(sum_sq))
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output1[1][0], output1[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(np.sqrt(sum_sq))
    
    break
38/70:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output1[1][0], output1[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output1[1][0], output1[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(np.sqrt(sum_sq))
    
    break
38/71:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output1[1][0], output1[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output1[1][0], output1[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
38/72:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
38/73:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/1:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/2:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100,3)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/3:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/4:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/5:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/6:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/7:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.shape())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/8:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/9:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([#transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/10:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.PILToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/11:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.PILToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/12:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.PILToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            print('14 -----------------')
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            print('15 -----------------')
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/13:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.PILToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            print('14 -----------------')
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            print('15 -----------------')
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/14:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.PILToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/15:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.PILToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        print(img0)
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
42/1:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        print(img0)
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
42/2:
thing = torch.Tensor([[1,2,3,],[4,5,6]])
thing
42/3:
thing = thing / 255
thing
41/16:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        print(img0.size())
        print(img0)
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/17:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        #print(img0.size())
        #print(img0)
        output1, output2 = net(img0, img1)
        print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/18:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        #print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        #print(img0.size())
        #print(img0)
        output1, output2 = net(img0, img1)
        #print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        #print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
42/4:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/19:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0

# Iterate throught the epochs
for epoch in range(25):
    #print('9.9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()
        #print('12 -----------------')
        # Pass in the two images into the network and obtain two outputs
        #print(img0.size())
        #print(img0)
        output1, output2 = net(img0, img1)
        #print('13 -----------------')
        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)
        #print('14 -----------------')
        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
41/20:
#model_scripted = torch.jit.script(net) # Export to TorchScript
#model_scripted.save('model_scripted.pt') # Save
41/21:
#model = torch.jit.load('model_scripted.pt')
#model.eval()
41/22:
#(img1, img2, three) = siamese_dataset.__getitem__(3)
#print(type(three)) # torch.Tensor
#print(three) # [1.] [0.]
41/23:
torch.save(net.state_dict(), 'my_model_save.pt')

model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
41/24:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/25:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    im = transforms.ToImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/26:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/27:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/28:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/29:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(lebel[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/30:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
41/31:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[1])
    break
41/32:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[1])
    break
41/33:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[1])
    break
41/34:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    '''
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    '''
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    #torch.save(net.state_dict(), 'my_model_save.pt')


    #model = SiameseNetwork()
    #model.load_state_dict(torch.load('my_model_save.pt'))
    #model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[1])
    break
41/35:
img = Image.open("./photo_to_find/trump.jpg")
img.show()
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print(type(folder_dataset))
print(folder_dataset)
41/36:
img = Image.open("./photo_to_find/trump.jpg")
print(img)
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print(type(folder_dataset))
print(folder_dataset)
41/37:
img = Image.open("./photo_to_find/trump.jpg")
print(img.show())
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print(type(folder_dataset))
print(folder_dataset)
41/38:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    # checking if it is a file
    if os.path.isfile(f):
        print(f)
41/39:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    # checking if it is a file
    print(f)
41/40:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    # checking if it is a file
    print(type(f))
    break
41/41:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(directory, filename)
        print(f2)
    break
41/42:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(directory, filename2)
        print(f2)
    break
41/43:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        print(f2)
    break
41/44:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        print(f2)
    counter += 1
    if counter == 5:
        break
41/45:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    print(f)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
41/46:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
print(img_input)
41/47:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)
41/48:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_input)
        im = transforms.ToPILImage()(img_compare).convert("RGB")
        display(im)
    break
41/49:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        im = transforms.ToPILImage()(img_compare).convert("RGB")
        display(im)
    break
41/50:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        im = transforms.ToPILImage()(img_compare).convert("RGB")
        display(im)
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        print(sum_sq)
    break
41/51:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        im = transforms.ToPILImage()(img_compare).convert("RGB")
        display(im)
        
        print(img_input.size())
        print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        print(sum_sq)
    break
41/52:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    print(sum_sq)
    
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
   
    print(output1[:4])
    print(output2[:4])
   
    output1, output2 = model(img0, img1)
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[1])
    break
41/53:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    print(sum_sq)
    
    im = transforms.ToPILImage()(img0[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[0]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img0[1]).convert("RGB")
    display(im)
    im = transforms.ToPILImage()(img1[1]).convert("RGB")
    display(im)
   
    #print(output1[:4])
    #print(output2[:4])
    print(img0.size())
    print(type(img0))
    output1, output2 = model(img0, img1)
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    print(sum_sq)
    print(label[0])
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    print(label[1])
    break
41/54:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten.size()
41/55:
z = torch.empty(64,1)
z[:, 0] = ten
print(z)
41/56:
z = torch.empty(64, 2, 2, 1)
z[:, 0] = ten
print(z)
41/57:
z = torch.empty(64, 2, 1)
z[:, 0] = ten
print(z)
41/58:
z = torch.empty(64, 2, 1)
z[:, 0] = ten
print(z)
41/59:
z = torch.empty(64, 2, 1)
z[:, 0, 0] = ten
print(z)
41/60:
z = torch.empty(64, 2, 1)
z[:, 0] = ten
print(z)
41/61:
z = torch.empty(64, 2, 1)
z[:] = ten
print(z)
41/62:
z = torch.empty(64, 2, 1)
z[:, 0, 0, 0] = ten
print(z)
41/63:
z = torch.empty(64, 2, 1)
z[:, 0, 0] = ten
print(z)
41/64:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten.expand(64, 2, 2, 1)
ten.size()
41/65:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten.expand(64, 2, 2, 1)
ten.size()
41/66:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten.expand(2, 2, 1, 64)
ten.size()
41/67:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten.expand(10, 2, 1)
ten.size()
41/68:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten.expand(2, 2, 2)
ten.size()
41/69:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten = ten.expand(2, 2, 2)
ten.size()
41/70:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten = ten.expand(2, 2, 2)
ten.size()
ten
41/71:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
ten
ten = ten.expand(2, 2, 2)
ten.size()
ten
41/72:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
ten = ten.expand(2, 2, 2)
ten.size()
ten
41/73:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./lfw_funneled/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)

for i in range(5):
    # Iterate over 5 images and test them with the first image (x0)
    _, x1, label2 = next(dataiter)
    
    output1, output2 = net(x0, x1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
41/74:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./lfw_funneled/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, num_workers=2, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)

for i, (img0, img1, label) in enumerate(test_dataloader, 0):
    
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(euclidean_distance)
    break
    #imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
41/75:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./lfw_funneled/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)
print('1')
for i, (img0, img1, label) in enumerate(test_dataloader, 0):
    print('2')
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(euclidean_distance)
    break
    #imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
41/76:
# Locate the test dataset and load it into the SiameseNetworkDataset
folder_dataset_test = datasets.ImageFolder(root="./lfw_funneled/")
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,
                                        transform=transformation)
test_dataloader = DataLoader(siamese_dataset, batch_size=1, shuffle=True)

# Grab one image that we are going to test
dataiter = iter(test_dataloader)
x0, _, _ = next(dataiter)
print('1')
for i, (img0, img1, label) in enumerate(test_dataloader, 0):
    print('2')
    print(img0.size())
    output1, output2 = net(img0, img1)
    euclidean_distance = F.pairwise_distance(output1, output2)
    print(euclidean_distance)
    break
    #imshow(torchvision.utils.make_grid(concatenated), f'Dissimilarity: {euclidean_distance.item():.2f}')
41/77:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
#ten = ten.expand(2, 2, 2)
ten.size()
arra = ten.numpy()
arra
41/78:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
#ten = ten.expand(2, 2, 2)
ten.size()
arra = ten.numpy()
new = torch.numpy([arra])
new.size()
41/79:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
#ten = ten.expand(2, 2, 2)
ten.size()
arra = ten.numpy()
new = torch.fromnumpy([arra])
new.size()
41/80:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
#ten = ten.expand(2, 2, 2)
ten.size()
arra = ten.numpy()
new = torch.from_numpy([arra])
new.size()
41/81:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
#ten = ten.expand(2, 2, 2)
ten.size()
arra = ten.numpy()
lis = arra.tolist()
new = torch.Tensor([to])
new.size()
41/82:
ten = torch.Tensor([[[1],[2]],[[3],[4]]])
print(ten)
#ten = ten.expand(2, 2, 2)
ten.size()
arra = ten.numpy()
lis = arra.tolist()
new = torch.Tensor([lis])
new.size()
41/83:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        im = transforms.ToPILImage()(img_compare).convert("RGB")
        display(im)
        
        #print(img_input.size()) torch.Size([3, 100, 100])
        #print(img_compare.size()) torch.Size([3, 100, 100])
        arra = img_input.numpy()
        lis = arra.tolist()
        img_input = torch.Tensor([lis])
        
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        print(sum_sq)
        
    break
41/84:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])

for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {filename2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
im = transforms.ToPILImage()(final_tensor).convert("RGB")
display(im)
41/85:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])

for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        print(img_input.size())
        print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {filename2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
im = transforms.ToPILImage()(final_tensor).convert("RGB")
display(im)
41/86:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
41/87:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        print(img_input.size())
        print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {filename2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
im = transforms.ToPILImage()(final_tensor).convert("RGB")
display(im)
41/88:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {filename2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
im = transforms.ToPILImage()(final_tensor).convert("RGB")
display(im)
41/89:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {filename2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
im = transforms.ToPILImage()(final_tensor).convert("RGB")
display(im)
41/90:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
41/91:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    im = transforms.ToPILImage()(img).convert("RGB")
    display(im)
44/1:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'model_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('model_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
44/2:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    
    torch.save(net.state_dict(), 'greymodel_save.pt')


    model = SiameseNetwork()
    model.load_state_dict(torch.load('greymodel_save.pt'))
    model.eval()
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
44/3:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
'''
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
'''
44/4:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/5:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/6:
torch.save(net.state_dict(), 'greymodel_save.pt')

model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
44/7:
#model_scripted = torch.jit.script(net) # Export to TorchScript
#model_scripted.save('model_scripted.pt') # Save
44/8:
#model = torch.jit.load('model_scripted.pt')
#model.eval()
44/9:
#(img1, img2, three) = siamese_dataset.__getitem__(3)
#print(type(three)) # torch.Tensor
#print(three) # [1.] [0.]
44/10:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/11:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    print(output1[:4])
    print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
44/12:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
44/13:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/14:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/15:
torch.save(net.state_dict(), 'greymodel_save.pt')

model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
44/16:
#model_scripted = torch.jit.script(net) # Export to TorchScript
#model_scripted.save('model_scripted.pt') # Save
44/17:
#model = torch.jit.load('model_scripted.pt')
#model.eval()
44/18:
#(img1, img2, three) = siamese_dataset.__getitem__(3)
#print(type(three)) # torch.Tensor
#print(three) # [1.] [0.]
44/19:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
44/20:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("grey")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
44/21:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("L")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
44/22:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("L")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
print(img_input.size())
44/23:
img_input = Image.open("./photo_to_find/trump.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("L")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
print(img_input.size())
44/24:
img_input = Image.open("./photo_to_find/trump.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
print(img_input.size())
44/25:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        im = transforms.ToPILImage()(img_compare).convert("L")
        display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    break
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    im = transforms.ToPILImage()(img).convert("grey")
    display(im)
45/1:
# import required module
import os
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("RGB")
    display(im)
45/2:
# import required module
import os
import torch
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("RGB")
    display(im)
45/3:
# import required module
import os
import torch
from PIL import Image
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("RGB")
    display(im)
45/4:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("RGB")
    display(im)
45/5:
torch.save(net.state_dict(), 'my_model_save.pt')

model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
45/6:
model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
45/7:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("RGB")
        img1 = img1.convert("RGB")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./lfw_funneled/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive
print('7 ---------------------------------------------------------------------------------------')
# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)
print('8 ---------------------------------------------------------------------------------------')
net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )
print('9 ---------------------------------------------------------------------------------------')
counter = []
loss_history = [] 
iteration_number= 0
45/8:
model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
45/9:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
45/10:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("RGB")
    display(im)
44/26:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("L")
    display(im)
44/27:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        print(img_input.size())
        print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("L")
    display(im)
44/28:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        print(f'input {img_input.size()}'')
        print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("L")
    display(im)
44/29:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        print(f'input {img_input.size()}')
        print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("L")
    display(im)
44/30:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img_compare)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("L")
    display(im)
44/31:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    transformation = transforms.Compose([transforms.ToTensor()])
    img = transformation(img)
    img_arra = img.numpy()
    im = transforms.ToPILImage()(img_arra).convert("L")
    display(im)
44/32:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
44/33:
img_input = Image.open("./photo_to_find/trump.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
print(img_input.size())
44/34:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
44/35:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    print(filename)
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
44/36:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'trump':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
44/37:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'trump':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{f2}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
45/11:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img_compare)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("RGB")
    #display(im)
45/12:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
45/13:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img_compare)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("RGB")
    #display(im)
44/38:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'trump':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
44/39:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/40:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/41:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(3, 64, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(64, 64, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(64, 128, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(128, 128, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(128, 256, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(256, 256, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(256, 256, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(256, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(384, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(385, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(384, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/42:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(64, 64, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(64, 128, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(128, 128, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(128, 256, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(256, 256, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(256, 256, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(256, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(384, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(385, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            nn.Conv2d(384, 384, 3, stride=1, padding=1)
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/43:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(385, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/44:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/45:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/46:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/47:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/48:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/49:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        #output = output.view(output.size()[0], -1)
        output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/50:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        #output = output.view(output.size()[0], -1)
        output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/51:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/52:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        #output = output.view(output.size()[0], -1)
        output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/53:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/54:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        #output = output.view(output.size()[0], -1)
        output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/55:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/56:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        #output = output.view(output.size()[0], -1)
        output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/57:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/58:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        #output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/59:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/60:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(13824, 6200),
            nn.ReLU(inplace=True),
            
            nn.Linear(6200, 3100),
            nn.ReLU(inplace=True),
            
            nn.Linear(3100, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        #output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=64)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/61:
# Iterate throught the epochs
for epoch in range(100):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
44/62:
#%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import random
from PIL import Image
import PIL.ImageOps    

import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.utils
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

# Showing images
def imshow(img, text=None):
    npimg = img.numpy()
    plt.axis("off")
    if text:
        plt.text(75, 8, text, style='italic',fontweight='bold',
            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})
        
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()    

# Plotting data
def show_plot(iteration,loss):
    plt.plot(iteration,loss)
    plt.show()
print('0 ---------------------------------------------------------------------------------------')
#---------------------------------------------------------------------------------------
class SiameseNetworkDataset(Dataset):
    def __init__(self,imageFolderDataset,transform=None):
        self.imageFolderDataset = imageFolderDataset    
        self.transform = transform
        
    def __getitem__(self,index):
        img0_tuple = random.choice(self.imageFolderDataset.imgs)

        #We need to approximately 50% of images to be in the same class
        should_get_same_class = random.randint(0,1) 
        if should_get_same_class:
            while True:
                #Look untill the same class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] == img1_tuple[1]:
                    break
        else:

            while True:
                #Look untill a different class image is found
                img1_tuple = random.choice(self.imageFolderDataset.imgs) 
                if img0_tuple[1] != img1_tuple[1]:
                    break

        img0 = Image.open(img0_tuple[0])
        img1 = Image.open(img1_tuple[0])

        img0 = img0.convert("L")
        img1 = img1.convert("L")

        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)
        
        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))
    
    def __len__(self):
        return len(self.imageFolderDataset.imgs)

print('1 ---------------------------------------------------------------------------------------')
# Load the training dataset
folder_dataset = datasets.ImageFolder(root="./data/faces/training/")
print('2 ---------------------------------------------------------------------------------------')
# Resize the images and transform to tensors
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
print('3 ---------------------------------------------------------------------------------------')
# Initialize the network
siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,
                                        transform=transformation)
print('4 ---------------------------------------------------------------------------------------')
# Create a simple dataloader just for simple visualization
vis_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        num_workers=2,
                        batch_size=8)

print('5 ---------------------------------------------------------------------------------------')
# Extract one batch
#example_batch = next(iter(vis_dataloader))
#print(example_batch)
# Example batch is a list containing 2x8 images, indexes 0 and 1, an also the label
# If the label is 1, it means that it is not the same person, label is 0, same person in both images
print('6 ---------------------------------------------------------------------------------------')
#concatenated = torch.cat((example_batch[0], example_batch[1]),0)

#imshow(torchvision.utils.make_grid(concatenated))
#print(example_batch[2].numpy().reshape(-1))

# create siameseNetwork
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(13824, 6200),
            nn.ReLU(inplace=True),
            
            nn.Linear(6200, 3100),
            nn.ReLU(inplace=True),
            
            nn.Linear(3100, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        #output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
    
# Define the Contrastive Loss Function
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
      # Calculate the euclidean distance and calculate the contrastive loss
      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)

      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))


      return loss_contrastive

# Load the training dataset
train_dataloader = DataLoader(siamese_dataset,
                        shuffle=True,
                        #num_workers=8,
                        batch_size=32)

net = SiameseNetwork()#.cuda()
criterion = ContrastiveLoss()
optimizer = optim.Adam(net.parameters(), lr = 0.0005 )

counter = []
loss_history = [] 
iteration_number= 0
44/63:
# Iterate throught the epochs
for epoch in range(20):
    #print('9 -----------------')
    # Iterate over batches
    for i, (img0, img1, label) in enumerate(train_dataloader, 0):
        #print('10 -----------------')
        
        # Send the images and labels to CUDA
        #img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()
        #print('11 -----------------')
        # Zero the gradients
        optimizer.zero_grad()

        # Pass in the two images into the network and obtain two outputs
        output1, output2 = net(img0, img1)

        # Pass the outputs of the networks and label into the loss function
        loss_contrastive = criterion(output1, output2, label)

        # Calculate the backpropagation
        loss_contrastive.backward()

        # Optimize
        optimizer.step()

        # Every 10 batches print out the loss
        if i % 10 == 0 :
            print(f"Epoch number {epoch}\n Current loss {loss_contrastive.item()}\n")
            iteration_number += 10

            counter.append(iteration_number)
            loss_history.append(loss_contrastive.item())

show_plot(counter, loss_history)
45/14:
model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
45/15:
img_input = Image.open("./photo_to_find/trump.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
45/16:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img_compare)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("RGB")
    #display(im)
44/64:
torch.save(net.state_dict(), 'greymodel_more_save.pt')

model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_more_save.pt'))
model.eval()
44/65:
#model_scripted = torch.jit.script(net) # Export to TorchScript
#model_scripted.save('model_scripted.pt') # Save
44/66:
#model = torch.jit.load('model_scripted.pt')
#model.eval()
44/67:
#(img1, img2, three) = siamese_dataset.__getitem__(3)
#print(type(three)) # torch.Tensor
#print(three) # [1.] [0.]
44/68:

for i, (img0, img1, label) in enumerate(train_dataloader, 0):
    output1, output2 = net(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)

    #happy_array = np.random.randn(3, 3)
    #print(type(happy_array)) numpy.ndarray
    #print(happy_array) [[1,2,3],[4,5,6],[7,8,9]]
    #print(type(img0)) torch.Tensor
    #print(img0.size()) [64, 1, 100, 100]
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    hundred_times_hundred = img0[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[0][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img0[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    
    hundred_times_hundred = img1[1][0]
    im = plt.imshow(hundred_times_hundred, cmap='gray', interpolation='none')
    cbar = plt.colorbar(im)
    plt.show()
    #print(euclidean_distance.size()) [64]
    #print(output1.size()) [64, 2]
    #print(euclidean_distance.item())
    #print(euclidean_distance[:4])
    #print(output1[:4])
    #print(output2[:4])
    #print(torch.sum(euclidean_distance).item())
    
    
    output1, output2 = model(img0, img1)
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[0][0], output1[0][1]))
    point2 = np.array((output2[0][0], output2[0][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    #------------------ second pair
    #euclidean_distance = F.pairwise_distance(output1, output2)
    #print(torch.sum(euclidean_distance).item())
    
    point1 = np.array((output1[1][0], output1[1][1]))
    point2 = np.array((output2[1][0], output2[1][1]))

    # finding sum of squares
    sum_sq = np.sum(np.square(point1 - point2))

    # Doing squareroot and
    # printing Euclidean distance
    print(sum_sq)
    
    break
44/69:
img_input = Image.open("./photo_to_find/trump.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
44/70:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'trump':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
44/71:
img_input = Image.open("./photo_to_find/input.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
44/72:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
44/73:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("L")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(f'input {img_input.size()}')
        #print(f'comp {img_compare.size()}')
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("L")
    #display(im)
45/17:
img_input = Image.open("./photo_to_find/input.jpeg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
45/18:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img_compare)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("RGB")
    #display(im)
45/19:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img_compare)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("RGB")
    #display(im)
45/20:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        #im = transforms.ToPILImage()(img_compare).convert("RGB")
        #display(im)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        #print(img_input.size())
        #print(img_compare.size())
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
    #transformation = transforms.Compose([transforms.ToTensor()])
    #img = transformation(img_compare)
    #img_arra = img.numpy()
    #im = transforms.ToPILImage()(img_arra).convert("RGB")
    #display(im)
46/1:
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/2:
import torch.nn as nn
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/3:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/4:
import torch.nn as nn
import torch
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/5:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/6:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/7:
import torch.nn as nn
import torch
from PIL import Image
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/8:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/9:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/10:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/11:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/12:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/13:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/14:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np
class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/15:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/16:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/17:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/18:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/19:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/20:
model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
46/21:
img_input = Image.open("./photo_to_find/input.jpeg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/22:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}

for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/23: # grey, github siamese dataset, vgg cnn
46/24:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(13824, 6200),
            nn.ReLU(inplace=True),
            
            nn.Linear(6200, 3100),
            nn.ReLU(inplace=True),
            
            nn.Linear(3100, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        #output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/25:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_more_save.pt'))
model.eval()
46/26:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/27:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/28:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/29:
model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
46/30:
img_input = Image.open("./photo_to_find/input.jpeg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/31:
img_input = Image.open("./photo_to_find/input.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/32:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}

for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/33:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            #nn.Conv2d(1, 96, kernel_size=11,stride=4),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(3, stride=2),
            
            #nn.Conv2d(96, 256, kernel_size=5, stride=1),
            #nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),

            #nn.Conv2d(256, 384, kernel_size=3,stride=1),
            #nn.ReLU(inplace=True)
            
            nn.Conv2d(1, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 64, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(64, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(128, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            #nn.MaxPool2d(2, stride=2),
            nn.Conv2d(256, 256, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            
            nn.Conv2d(256, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(384, 384, 3, stride=1, padding=1),
            nn.ReLU(inplace=True)
            #self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
            #self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)
            #nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(13824, 6200),
            nn.ReLU(inplace=True),
            
            nn.Linear(6200, 3100),
            nn.ReLU(inplace=True),
            
            nn.Linear(3100, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        #output = torch.flatten(output, 1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/34:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_more_save.pt'))
model.eval()
46/35:
img_input = Image.open("./photo_to_find/input.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/36:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/37:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/38:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/39:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/40:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/41:
img_input = Image.open("./photo_to_find/input.jpg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/42:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
47/1:
import torch
import torch.nn.functional as F
import torch.nn as nn

MEAN_RGB = [
    0.367035294117647,
    0.41083294117647057,
    0.5066129411764705
]


def vggface(pretrained=False, **kwargs):
    """VGGFace model.
    Args:
        pretrained (bool): If True, returns pre-trained model 
    """
    model = VggFace(**kwargs)
    if pretrained:
        model.load_state_dict(torch.load('vggfacepreprepared.pth'))
    return model


class VggFace(torch.nn.Module):
    def __init__(self, classes=2622):
        """VGGFace model.
        Face recognition network.  It takes as input a Bx3x224x224
        batch of face images and gives as output a BxC score vector
        (C is the number of identities).
        Input images need to be scaled in the 0-1 range and then 
        normalized with respect to the mean RGB used during training.
        Args:
            classes (int): number of identities recognized by the
            network
        """
        super().__init__()
        self.conv1 = _ConvBlock(3, 64, 64)
        self.conv2 = _ConvBlock(64, 128, 128)
        self.conv3 = _ConvBlock(128, 256, 256, 256)
        self.conv4 = _ConvBlock(256, 512, 512, 512)
        self.conv5 = _ConvBlock(512, 512, 512, 512)
        self.dropout = torch.nn.Dropout(0.5)
        self.fc1 = torch.nn.Linear(7 * 7 * 512, 4096)
        self.fc2 = torch.nn.Linear(4096, 4096)
        self.fc3 = torch.nn.Linear(4096, classes)
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.fc3(x)
        return x


class _ConvBlock(torch.nn.Module):
    """A Convolutional block."""

    def __init__(self, *units):
        """Create a block with len(units) - 1 convolutions.
        convolution number i transforms the number of channels from 
        units[i - 1] to units[i] channels.
        """
        super().__init__()
        self.convs = torch.nn.ModuleList([
            torch.nn.Conv2d(in_, out, 3, 1, 1)
            for in_, out in zip(units[:-1], units[1:])
        ])
        
    def forward(self, x):
        # Each convolution is followed by a ReLU, then the block is
        # concluded by a max pooling.
        for c in self.convs:
            x = F.relu(c(x))
        return F.max_pool2d(x, 2, 2, 0, ceil_mode=True)
    

def _test_image(net, names, im):
    import torchvision
    tr = torchvision.transforms.Compose([
        torchvision.transforms.Resize((224, 224)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(MEAN_RGB, (1, 1, 1))
    ])
    x = tr(im)[None, ...]
    x = net(x)
    y = torch.nn.functional.softmax(x, 1)
    print(y)
    N = 5
    rank = torch.topk(y[0, :], N)
    print(rank)
    for i in range(N):
        index = rank[1][i].item()
        score = rank[0][i].item()
        print("{}) {} ({:.2f})".format(i + 1, names[index], score))
    

def _test():
    import sys
    from PIL import Image
    net = vggface(True)
    net.eval()
    namepaths=['test-images/sean-bean.jpg', 'test-images/chris-hemsworth.jpg']
    names = open("names.txt").read().split()
    with torch.no_grad():
        for path in namepaths:
            print(path)
            _test_image(net, names, Image.open(path))
            print()
47/2:
class newVGGFace(nn.Module):
    def __init__(self, in_channels, num_classes=2622, init_weights=True):
        super(newVGGFace, self).__init__()
        self.init_weights = init_weights
        self.in_channels = in_channels
        self.num_classes = num_classes
        # convolutional layers 
        self.conv_layers = nn.Sequential(
            # 1
            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #5
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)
        )

        # fully connected linear layers
        self.linear_layers = nn.Sequential(
            nn.Linear(in_features=512*7*7, out_features=4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=4096, out_features=4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=4096, out_features=self.num_classes)
        )

        if init_weights:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)
    

    def forward(self, x):
        x = self.conv_layers(x)
        # flatten to prepare for the fully connected layers
        x = x.view(x.size(0), -1)
        x = self.linear_layers(x)
        return x
47/3:
vggtrained = vggface(pretrained=True)
nontrainedVgg = newVGGFace(3)
47/4:
# Copy over weights
# 1
nontrainedVgg.conv_layers[0].weight.data.copy_(vggtrained.conv1.convs[0].weight.data)
nontrainedVgg.conv_layers[0].bias.data.copy_(vggtrained.conv1.convs[0].bias.data)

nontrainedVgg.conv_layers[2].weight.data.copy_(vggtrained.conv1.convs[1].weight.data)
nontrainedVgg.conv_layers[2].bias.data.copy_(vggtrained.conv1.convs[1].bias.data)

# 2
nontrainedVgg.conv_layers[5].weight.data.copy_(vggtrained.conv2.convs[0].weight.data)
nontrainedVgg.conv_layers[5].bias.data.copy_(vggtrained.conv2.convs[0].bias.data)

nontrainedVgg.conv_layers[7].weight.data.copy_(vggtrained.conv2.convs[1].weight.data)
nontrainedVgg.conv_layers[7].bias.data.copy_(vggtrained.conv2.convs[1].bias.data)

#3
nontrainedVgg.conv_layers[10].weight.data.copy_(vggtrained.conv3.convs[0].weight.data)
nontrainedVgg.conv_layers[10].bias.data.copy_(vggtrained.conv3.convs[0].bias.data)

nontrainedVgg.conv_layers[12].weight.data.copy_(vggtrained.conv3.convs[1].weight.data)
nontrainedVgg.conv_layers[12].bias.data.copy_(vggtrained.conv3.convs[1].bias.data)

nontrainedVgg.conv_layers[14].weight.data.copy_(vggtrained.conv3.convs[2].weight.data)
nontrainedVgg.conv_layers[14].bias.data.copy_(vggtrained.conv3.convs[2].bias.data)

#4
nontrainedVgg.conv_layers[17].weight.data.copy_(vggtrained.conv4.convs[0].weight.data)
nontrainedVgg.conv_layers[17].bias.data.copy_(vggtrained.conv4.convs[0].bias.data)

nontrainedVgg.conv_layers[19].weight.data.copy_(vggtrained.conv4.convs[1].weight.data)
nontrainedVgg.conv_layers[19].bias.data.copy_(vggtrained.conv4.convs[1].bias.data)

nontrainedVgg.conv_layers[21].weight.data.copy_(vggtrained.conv4.convs[2].weight.data)
nontrainedVgg.conv_layers[21].bias.data.copy_(vggtrained.conv4.convs[2].bias.data)

#5
nontrainedVgg.conv_layers[24].weight.data.copy_(vggtrained.conv5.convs[0].weight.data)
nontrainedVgg.conv_layers[24].bias.data.copy_(vggtrained.conv5.convs[0].bias.data)

nontrainedVgg.conv_layers[26].weight.data.copy_(vggtrained.conv5.convs[1].weight.data)
nontrainedVgg.conv_layers[26].bias.data.copy_(vggtrained.conv5.convs[1].bias.data)

nontrainedVgg.conv_layers[28].weight.data.copy_(vggtrained.conv5.convs[2].weight.data)
nontrainedVgg.conv_layers[28].bias.data.copy_(vggtrained.conv5.convs[2].bias.data)

# Linear layers
nontrainedVgg.linear_layers[0].weight.data.copy_(vggtrained.fc1.weight.data)
nontrainedVgg.linear_layers[0].bias.data.copy_(vggtrained.fc1.bias.data)

nontrainedVgg.linear_layers[3].weight.data.copy_(vggtrained.fc2.weight.data)
nontrainedVgg.linear_layers[3].bias.data.copy_(vggtrained.fc2.bias.data)

nontrainedVgg.linear_layers[6].weight.data.copy_(vggtrained.fc3.weight.data)
nontrainedVgg.linear_layers[6].bias.data.copy_(vggtrained.fc3.bias.data)
47/5:
# Test that we copied weights across
print(torch.all(nontrainedVgg.conv_layers[0].weight.data.eq(vggtrained.conv1.convs[0].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[0].bias.data.eq(vggtrained.conv1.convs[0].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[2].weight.data.eq(vggtrained.conv1.convs[1].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[2].bias.data.eq(vggtrained.conv1.convs[1].bias.data)))

# 2
print(torch.all(nontrainedVgg.conv_layers[5].weight.data.eq(vggtrained.conv2.convs[0].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[5].bias.data.eq(vggtrained.conv2.convs[0].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[7].weight.data.eq(vggtrained.conv2.convs[1].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[7].bias.data.eq(vggtrained.conv2.convs[1].bias.data)))

#3
print(torch.all(nontrainedVgg.conv_layers[10].weight.data.eq(vggtrained.conv3.convs[0].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[10].bias.data.eq(vggtrained.conv3.convs[0].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[12].weight.data.eq(vggtrained.conv3.convs[1].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[12].bias.data.eq(vggtrained.conv3.convs[1].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[14].weight.data.eq(vggtrained.conv3.convs[2].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[14].bias.data.eq(vggtrained.conv3.convs[2].bias.data)))

#4
print(torch.all(nontrainedVgg.conv_layers[17].weight.data.eq(vggtrained.conv4.convs[0].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[17].bias.data.eq(vggtrained.conv4.convs[0].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[19].weight.data.eq(vggtrained.conv4.convs[1].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[19].bias.data.eq(vggtrained.conv4.convs[1].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[21].weight.data.eq(vggtrained.conv4.convs[2].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[21].bias.data.eq(vggtrained.conv4.convs[2].bias.data)))

#5
print(torch.all(nontrainedVgg.conv_layers[24].weight.data.eq(vggtrained.conv5.convs[0].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[24].bias.data.eq(vggtrained.conv5.convs[0].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[26].weight.data.eq(vggtrained.conv5.convs[1].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[26].bias.data.eq(vggtrained.conv5.convs[1].bias.data)))

print(torch.all(nontrainedVgg.conv_layers[28].weight.data.eq(vggtrained.conv5.convs[2].weight.data)))
print(torch.all(nontrainedVgg.conv_layers[28].bias.data.eq(vggtrained.conv5.convs[2].bias.data)))

# Linear layers
print(torch.all(nontrainedVgg.linear_layers[0].weight.data.eq(vggtrained.fc1.weight.data)))
print(torch.all(nontrainedVgg.linear_layers[0].bias.data.eq(vggtrained.fc1.bias.data)))

print(torch.all(nontrainedVgg.linear_layers[3].weight.data.eq(vggtrained.fc2.weight.data)))
print(torch.all(nontrainedVgg.linear_layers[3].bias.data.eq(vggtrained.fc2.bias.data)))

print(torch.all(nontrainedVgg.linear_layers[6].weight.data.eq(vggtrained.fc3.weight.data)))
print(torch.all(nontrainedVgg.linear_layers[6].bias.data.eq(vggtrained.fc3.bias.data)))
47/6: torch.save(nontrainedVgg.state_dict(), './vggfaceweights.pth')
40/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np
import torchvision
import matplotlib.pyplot as plt

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")


class VGGFace(nn.Module):
    def __init__(self, in_channels, num_classes=2622, init_weights=True):
        super(VGGFace, self).__init__()
        self.init_weights = init_weights
        self.in_channels = in_channels
        self.num_classes = num_classes
        # convolutional layers 
        self.conv_layers = nn.Sequential(
            # 1
            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #2
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #3
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #4
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),
            #5
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)
        )

        # fully connected linear layers
        self.linear_layers = nn.Sequential(
            nn.Linear(in_features=512*7*7, out_features=4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=4096, out_features=4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(in_features=4096, out_features=self.num_classes)
        )

        if init_weights:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)
    

    def forward(self, x):
        x = self.conv_layers(x)
        # flatten to prepare for the fully connected layers
        x = x.view(x.size(0), -1)
        x = self.linear_layers(x)
        return x

MEAN_RGB = [
    0.367035294117647,
    0.41083294117647057,
    0.5066129411764705
]
40/2:
# Load Model
model = VGGFace(3, init_weights=False)
model.load_state_dict(torch.load('preprepared/vggfaceweights.pth'))
40/3:
import sys
from PIL import Image

def _test_image(net, names, im):
    tr = torchvision.transforms.Compose([
        torchvision.transforms.Resize((224, 224)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(MEAN_RGB, (1, 1, 1))
    ])
    x = tr(im)[None, ...]
    x = net(x)
    y = torch.nn.functional.softmax(x, 1)
    N = 5
    rank = torch.topk(y[0, :], N)
    print(rank)
    for i in range(N):
        index = rank[1][i].item()
        score = rank[0][i].item()
        print("{}) {} ({:.2f})".format(i + 1, names[index], score))
    

net = model
net.eval()
namepaths=['preprepared/test-images/andrew-garfield.jpg', 'preprepared/test-images/chris-hemsworth.jpg']
names = open("preprepared/names.txt").read().split()
with torch.no_grad():
    for path in namepaths:
        print(path)
        _test_image(net, names, Image.open(path))
        print()
46/43:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(3, 96, kernel_size=11,stride=4),
            
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
           
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/44:
model = SiameseNetwork()
model.load_state_dict(torch.load('my_model_save.pt'))
model.eval()
46/45:
img_input = Image.open("./photo_to_find/input.jpg")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input).convert("RGB")
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/46:
# import required module
import os
import torch
from PIL import Image
import torchvision.transforms as transforms
# assign directory
directory = './lfw_funneled/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}

for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)
        
        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
        
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
46/47:
import torch.nn as nn
import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np

class SiameseNetwork(nn.Module):

    def __init__(self):
        super(SiameseNetwork, self).__init__()

        # Setting up the Sequential of CNN Layers
        self.cnn1 = nn.Sequential(
            nn.Conv2d(1, 96, kernel_size=11,stride=4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3, stride=2),
            
            nn.Conv2d(96, 256, kernel_size=5, stride=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2),

            nn.Conv2d(256, 384, kernel_size=3,stride=1),
            nn.ReLU(inplace=True)
        )

        # Setting up the Fully Connected Layers
        self.fc1 = nn.Sequential(
            nn.Linear(384, 1024),
            nn.ReLU(inplace=True),
            
            nn.Linear(1024, 256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256,2)
        )
        
    def forward_once(self, x):
        # This function will be called for both images
        # Its output is used to determine the similiarity
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        # In this function we pass in both images and obtain both vectors
        # which are returned
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)

        return output1, output2
46/48:
model = SiameseNetwork()
model.load_state_dict(torch.load('greymodel_save.pt'))
model.eval()
46/49:
img_input = Image.open("./photo_to_find/input.jpeg")
img_input = img_input.convert("L")
transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()
                                    ])
img_input = transformation(img_input)
im = transforms.ToPILImage()(img_input)
display(im)

arra = img_input.numpy()
lis = arra.tolist()
img_input = torch.Tensor([lis])
46/50:
# import required module
import os
# assign directory
directory = './data/faces/all/'
 
# iterate over files in
# that directory
counter = 0
final_name_dict = {'first_person': 99}
final_tensor = torch.Tensor()


for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    if os.path.isfile(f):
        continue
    for filename2 in os.listdir(f):
        f2 = os.path.join(f, filename2)
        
        img_compare = Image.open(f"{f2}")
        img_compare = img_compare.convert('L')
        transformation = transforms.Compose([transforms.Resize((100,100)),
                                     transforms.ToTensor()])
        img_compare = transformation(img_compare)

        arra = img_compare.numpy()
        lis = arra.tolist()
        img_compare = torch.Tensor([lis])
        
        output1, output2 = model(img_input, img_compare)

        point1 = np.array((output1[0][0], output1[0][1]))
        point2 = np.array((output2[0][0], output2[0][1]))

        sum_sq = np.sum(np.square(point1 - point2))
        if filename == 'danver':
            print(sum_sq)
        for key in final_name_dict:
            if final_name_dict[key] > sum_sq:
                final_name_dict = {f2: sum_sq} 
                final_tensor = img_compare
    
        
print(final_name_dict)
for key in final_name_dict:
    img = Image.open(f"{key}")
    img.show()
   1: %history -g -f history.txt
