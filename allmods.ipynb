{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction or Motivation or Problem Statement\n",
    "We are trying to train a facial recognition neural network. To be able to identify people in pictures \n",
    "with the people inside a database.\n",
    "\n",
    "\n",
    "# Data Sources\n",
    "The data used for training can be found at:\n",
    "AT&T dataset - https://github.com/maticvl/dataHacker/blob/master/DATA/at%26t.zip \n",
    "(gray-scaled dataset with 400 samples)\n",
    "lfw dataset - https://www.kaggle.com/datasets/atulanandjha/lfwpeople\n",
    "(colored dataset with 13000+ images)\n",
    "\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "Each class in the dataset is a person's name with 10 grey pictures of their portrait.\n",
    "For data preparation the image transformed to size 100 x 100 to fit the model and changed into grey pictures and then turned into 1x100x100 tensor.\n",
    "In SiameseNetworkDataset's getitem it choices at 50/50 chance to be in the same class or different class and by checking if's their folder's name is the same it produces a lebel of 1 or 0.\n",
    "\n",
    "\n",
    "# Model\n",
    "At the start we just wanted to make a basic facial recognition neural network to work, that's why we chose Siamese with contrastive loss with grey pictures, it is easy to understant and easy to implement.\n",
    "The model used is the Siamese neural network, it has inputs as 2 images. Both images will pass through the same convolution neural network with fully connected layers at the end to produce 2 values as output. And the euclidean distance between the two images is the dissimilaity score (because both pictures went through the same neural network and if the images are in the same class it should have small distance score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import PIL\n",
    "# grey github dataset, vgg cnn\n",
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(1, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(384, 384, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Setting up the Fully Connected Layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(13824, 6200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(6200, 3100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(3100, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(256,2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # This function will be called for both images\n",
    "        # Its output is used to determine the similiarity\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        #output = torch.flatten(output, 1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # In this function we pass in both images and obtain both vectors\n",
    "        # which are returned\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (cnn1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (18): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=13824, out_features=6200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=6200, out_features=3100, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=3100, out_features=1024, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load('greymodel_more_colab_save100.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100 at 0x7FCEC9368BE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_input = Image.open(\"./photo_to_find/input.jpeg\")\n",
    "img_input = img_input.convert(\"L\")\n",
    "\n",
    "transformation = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                     transforms.ToTensor()\n",
    "                                    ])\n",
    "img_input = transformation(img_input)\n",
    "\n",
    "im = transforms.ToPILImage()(img_input)\n",
    "display(im)\n",
    "\n",
    "arra = img_input.numpy()\n",
    "lis = arra.tolist()\n",
    "img_input = torch.Tensor([lis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 pictures compared\n",
      "200 pictures compared\n",
      "300 pictures compared\n",
      "tensor(16.2426, grad_fn=<AddBackward0>)\n",
      "tensor(23.3760, grad_fn=<AddBackward0>)\n",
      "tensor(17.5352, grad_fn=<AddBackward0>)\n",
      "400 pictures compared\n",
      "time used to search: 0:01:33.368601\n",
      "[{'./data/faces/all/database/danver_1.jpg': 0.2902}, {'./data/faces/all/database/danver_3.jpg': 0.012}, {'./data/faces/all/database/aiony-haust.jpg': 0.7236}, {'./data/faces/all/s26/1.pgm': 1.7729}, {'./data/faces/all/s28/10.pgm': 2.031}, {'./data/faces/all/s12/7.pgm': 1.0677}, {'./data/faces/all/s12/4.pgm': 1.201}, {'./data/faces/all/s12/5.pgm': 1.5403}, {'./data/faces/all/s12/2.pgm': 1.2678}, {'./data/faces/all/s12/3.pgm': 1.8177}]\n",
      "lowest distance pic is ./data/faces/all/database/danver_3.jpg at distance 0.012\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import os\n",
    "import datetime\n",
    "# assign directory\n",
    "directory = './data/faces/all/'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "counter = 0\n",
    "final_name_dict = {'first_person': 99}\n",
    "top_10_score = 99\n",
    "final_list = []\n",
    "final_tensor = torch.Tensor()\n",
    "\n",
    "pic_counter = 0\n",
    "start = datetime.datetime.now()\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        continue\n",
    "    for filename2 in os.listdir(f):\n",
    "        f2 = os.path.join(f, filename2)\n",
    "        \n",
    "        img_compare = Image.open(f\"{f2}\")\n",
    "        img_compare = img_compare.convert('L')\n",
    "        transformation = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                     transforms.ToTensor()])\n",
    "        img_compare = transformation(img_compare)\n",
    "\n",
    "        arra = img_compare.numpy()\n",
    "        lis = arra.tolist()\n",
    "        img_compare = torch.Tensor([lis])\n",
    "        \n",
    "        output1, output2 = model(img_input, img_compare)\n",
    "\n",
    "        point1 = np.array((output1[0][0], output1[0][1]))\n",
    "        point2 = np.array((output2[0][0], output2[0][1]))\n",
    "\n",
    "        sum_sq = np.sum(np.square(point1 - point2))\n",
    "        pic_counter += 1\n",
    "        if pic_counter % 100 == 0:\n",
    "            print(f'{pic_counter} pictures compared')\n",
    "        if filename == 'check':\n",
    "            print(sum_sq)\n",
    "        \n",
    "        \n",
    "        if len(final_list) < 10 and sum_sq < top_10_score:\n",
    "            final_list.append({f2: round(sum_sq.item(), 4)})\n",
    "        if len(final_list) >= 10 and sum_sq < top_10_score:\n",
    "            # find max score in the final list and get rid of it\n",
    "            final_list.append({f2: round(sum_sq.item(), 4)})\n",
    "            max_score = 0.00\n",
    "            max_key = ''\n",
    "            max_thing = {}\n",
    "            for thing in final_list:\n",
    "                for key in thing:\n",
    "                    if thing[key] > max_score:\n",
    "                        max_score = thing[key]\n",
    "                        max_key = key\n",
    "                        max_thing = thing\n",
    "                        \n",
    "            top_10_score = max_score\n",
    "            final_list.remove(max_thing)\n",
    "            \n",
    "        # #if only want the lowest score\n",
    "        # for key in final_name_dict:\n",
    "        #     if final_name_dict[key] > sum_sq:\n",
    "        #         final_name_dict = {f2: sum_sq} \n",
    "        #         final_tensor = img_compare\n",
    "\n",
    "\n",
    "print(f'time used to search: {datetime.datetime.now() - start}') \n",
    "# top 10 lowest score\n",
    "print(final_list)\n",
    "min_score = 99\n",
    "lowest_pic = ''\n",
    "for thing in final_list:\n",
    "    for key in thing:\n",
    "        if thing[key] < min_score:\n",
    "            lowest_pic = key\n",
    "            min_score = thing[key]\n",
    "\n",
    "img = Image.open(f\"{lowest_pic}\")\n",
    "img.show()\n",
    "print(f'lowest distance pic is {lowest_pic} at distance {min_score}')            \n",
    "\n",
    "\n",
    "# if 1 lowest\n",
    "#print(final_name_dict)\n",
    "# for key in final_name_dict:\n",
    "#     img = Image.open(f\"{key}\")\n",
    "#     img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfw dataset, github vgg cnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11,stride=4),\n",
    "            \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "           \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3,stride=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Setting up the Fully Connected Layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(384, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(256,2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # This function will be called for both images\n",
    "        # Its output is used to determine the similiarity\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # In this function we pass in both images and obtain both vectors\n",
    "        # which are returned\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load('my_model_save.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Image.open(\"./photo_to_find/purnjay2.jpg\")\n",
    "transformation = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                     transforms.ToTensor()])\n",
    "img_input = transformation(img_input)\n",
    "im = transforms.ToPILImage()(img_input).convert(\"RGB\")\n",
    "display(im)\n",
    "\n",
    "arra = img_input.numpy()\n",
    "lis = arra.tolist()\n",
    "img_input = torch.Tensor([lis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "# assign directory\n",
    "directory = './lfw_funneled/'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "counter = 0\n",
    "pic_counter = 0\n",
    "final_name_dict = {'first_person': 99}\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        continue\n",
    "    for filename2 in os.listdir(f):\n",
    "        f2 = os.path.join(f, filename2)\n",
    "        \n",
    "        img_compare = Image.open(f\"{f2}\")\n",
    "        transformation = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                     transforms.ToTensor()])\n",
    "        img_compare = transformation(img_compare)\n",
    "        \n",
    "        arra = img_compare.numpy()\n",
    "        lis = arra.tolist()\n",
    "        img_compare = torch.Tensor([lis])\n",
    "        \n",
    "        output1, output2 = model(img_input, img_compare)\n",
    "\n",
    "        point1 = np.array((output1[0][0], output1[0][1]))\n",
    "        point2 = np.array((output2[0][0], output2[0][1]))\n",
    "\n",
    "        sum_sq = np.sum(np.square(point1 - point2))\n",
    "        # filename starts with 5a's to have the file always be the firstl, easy to edit\n",
    "        if filename == 'aaaaa_input':\n",
    "            print(sum_sq)\n",
    "        pic_counter += 1\n",
    "        if pic_counter % 1000 == 0:\n",
    "            print(f'{pic_counter} pictures compared')\n",
    "        for key in final_name_dict:\n",
    "            if final_name_dict[key] > sum_sq:\n",
    "                final_name_dict = {f2: sum_sq} \n",
    "        \n",
    "        \n",
    "print(final_name_dict)\n",
    "print(datetime.datetime.now() - start)\n",
    "for key in final_name_dict:\n",
    "    img = Image.open(f\"{key}\")\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grey github dataset, github cnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=11,stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3,stride=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Setting up the Fully Connected Layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(384, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(256,2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # This function will be called for both images\n",
    "        # Its output is used to determine the similiarity\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # In this function we pass in both images and obtain both vectors\n",
    "        # which are returned\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load('greymodel_save.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Image.open(\"./photo_to_find/purnjay2.jpg\")\n",
    "img_input = img_input.convert(\"L\")\n",
    "transformation = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                     transforms.ToTensor()\n",
    "                                    ])\n",
    "img_input = transformation(img_input)\n",
    "im = transforms.ToPILImage()(img_input)\n",
    "display(im)\n",
    "\n",
    "arra = img_input.numpy()\n",
    "lis = arra.tolist()\n",
    "img_input = torch.Tensor([lis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "# assign directory\n",
    "directory = './data/faces/all/'\n",
    " \n",
    "# iterate over files in\n",
    "# that directory\n",
    "counter = 0\n",
    "final_name_dict = {'first_person': 99}\n",
    "final_tensor = torch.Tensor()\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        continue\n",
    "    for filename2 in os.listdir(f):\n",
    "        f2 = os.path.join(f, filename2)\n",
    "        \n",
    "        img_compare = Image.open(f\"{f2}\")\n",
    "        img_compare = img_compare.convert('L')\n",
    "        transformation = transforms.Compose([transforms.Resize((100,100)),\n",
    "                                     transforms.ToTensor()])\n",
    "        img_compare = transformation(img_compare)\n",
    "\n",
    "        arra = img_compare.numpy()\n",
    "        lis = arra.tolist()\n",
    "        img_compare = torch.Tensor([lis])\n",
    "        \n",
    "        output1, output2 = model(img_input, img_compare)\n",
    "\n",
    "        point1 = np.array((output1[0][0], output1[0][1]))\n",
    "        point2 = np.array((output2[0][0], output2[0][1]))\n",
    "\n",
    "        sum_sq = np.sum(np.square(point1 - point2))\n",
    "        if filename == 'check':\n",
    "            print(sum_sq)\n",
    "        for key in final_name_dict:\n",
    "            if final_name_dict[key] > sum_sq:\n",
    "                final_name_dict = {f2: sum_sq} \n",
    "                final_tensor = img_compare\n",
    "    \n",
    "        \n",
    "print(final_name_dict)\n",
    "for key in final_name_dict:\n",
    "    img = Image.open(f\"{key}\")\n",
    "    img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
